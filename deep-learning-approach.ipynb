{
  "cells": [
    {
      "metadata": {
        "_uuid": "b12593dbf485cbab08077fbf6f76afce4b678ebd"
      },
      "cell_type": "markdown",
      "source": "# Using Convolutional Neural Network for Text Classification"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "251261c1f546a58f9798d28eacbe6d9791d75468"
      },
      "cell_type": "markdown",
      "source": "## Getting Started\nThanks to wonderful python libraries and packages for making our life easier."
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# nlp\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize \nfrom gensim.models.word2vec import Word2Vec\nimport spacy\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom keras.preprocessing import sequence\n\n#utils\nfrom collections import Counter, defaultdict\nimport gc, time\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.utils import class_weight\nimport functools\nfrom keras import backend as K\nimport tensorflow as tf\n\n\n#visualization\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom tabulate import tabulate\nimport seaborn as sns\n\n# some basic ml models and metrics evaluation\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom scikitplot.metrics import plot_confusion_matrix\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.utils.fixes import signature\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import mean_squared_error\n\n# deep learning\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import model_from_json\nfrom keras.models import Model\nfrom keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, Conv1D\nfrom keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\nfrom keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\nfrom keras.initializers import *\nfrom keras import regularizers\n\n\nimport tensorflow as tf\nfrom keras import backend as K\nimport random as rn\nseed = 42\nnp.random.seed(seed)\nrn.seed(395180390400)\ntf.set_random_seed(395180390400)\nsession_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\nsess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\nK.set_session(sess)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6c11c29912c9bd099637ecd3e79c4a4746b6f3d8"
      },
      "cell_type": "markdown",
      "source": "Let's get some idea about the data that we are dealing."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7c9fdc58363e56e100df64bbf5a7699561a79b76"
      },
      "cell_type": "code",
      "source": "train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n# Looking the data\nprint(\"Train shape : \", train.shape)\nprint(\"Test shape : \", test.shape)\ndist = train['target'].value_counts()\nsns.barplot(x=np.arange(2), y=dist)\nplt.title(\"Distribution of positive and negative labels\")\nplt.xlabel(\"target\")\nplt.ylabel(\"Count\")\nplt.show()\n# train.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f1c23aa57052775508098a2a433ad39cc42682fe"
      },
      "cell_type": "code",
      "source": "train_text = train['question_text']\ntest_text = test['question_text']\nall_text = pd.concat([train_text, test_text])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5bd1f06d0add6accfc3e02c2d58e7089a68805fc"
      },
      "cell_type": "markdown",
      "source": "## Utils"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "16ea721b5753841b3aed97b7211434d9a2f515b7"
      },
      "cell_type": "code",
      "source": "def evaluatePredictions(y, pred, silent=False):\n    f1_list = list()\n    thre_list = np.arange(0.1, 0.901, 0.01)\n    for thresh in thre_list:\n        thresh = np.round(thresh, 2)\n        f1 = f1_score(y, (pred>thresh).astype(int))\n        f1_list.append(f1)\n        if not silent:\n            print(\"F1 score at threshold {0} is {1}\".format(thresh, f1))\n    #return f1_list\n    plot_confusion_matrix(y, np.array(pd.Series(pred.reshape(-1,)).map(lambda x:1 if x>thre_list[np.argmax(f1_list)] else 0)))\n    best = thre_list[np.argmax(f1_list)]\n    best = np.round(best, 2)\n    print('Best Threshold: ', best)\n    print('Best F1 Score: ', np.max(f1_list))\n    return best\n\ndef plotPrecisionRecall(y, pred):\n    precision, recall, _ = precision_recall_curve(y, pred)\n    # In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n    step_kwargs = ({'step': 'post'}\n                   if 'step' in signature(plt.fill_between).parameters\n                   else {})\n    plt.step(recall, precision, color='b', alpha=0.2,\n             where='post')\n    plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.ylim([0.0, 1.05])\n    plt.xlim([0.0, 1.0])\n    average_precision = average_precision_score(y, pred)\n    plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n              average_precision))\n    \n # sparse to seq converter   \ndef to_seq(X):\n    ptrs = X.indptr\n    indices = X.indices\n    seq = []\n    prev = 0\n    max_words = 0\n    for i in range(1, len(ptrs)):\n        current = ptrs[i]\n        words = list(indices[prev:current])\n        length = len(words)\n        if length > max_words:\n            max_words = length\n        seq.append(words)\n        prev = current\n    print(\"Maximum words = \", max_words)\n    return seq\n    \nstop_words = stopwords.words('english')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "31b1626223ed4c5e55072e7ff300c53f40b7ce42"
      },
      "cell_type": "code",
      "source": "def plot_history(history, measure='acc'):\n    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n    acc_list = [s for s in history.history.keys() if measure in s and 'val' not in s]\n    val_acc_list = [s for s in history.history.keys() if measure in s and 'val' in s]\n    \n    if len(loss_list) == 0:\n        print('Loss is missing in history')\n        return \n    \n    ## As loss always exists\n    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n    \n    ## Loss\n    plt.figure(1)\n    for l in loss_list:\n        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n    for l in val_loss_list:\n        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n    \n    plt.title('Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    ## Accuracy\n    plt.figure(2)\n    for l in acc_list:\n        plt.plot(epochs, history.history[l], 'b', label='Training {} ('.format(measure) + str(format(history.history[l][-1],'.5f'))+')')\n    for l in val_acc_list:    \n        plt.plot(epochs, history.history[l], 'g', label='Validation {} ('.format(measure) + str(format(history.history[l][-1],'.5f'))+')')\n\n    plt.title(measure)\n    plt.xlabel('Epochs')\n    plt.ylabel(measure)\n    plt.legend()\n    plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a6936d44b23b4750336dd29d796634b70ebeef84"
      },
      "cell_type": "markdown",
      "source": "## Loading Embeddings"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ce92224816a6f4bccb20be623619763cefb607db"
      },
      "cell_type": "code",
      "source": "glove_em_file = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n\ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\nembedding_index = dict(get_coefs(*d.split(' ')) for d in open(glove_em_file))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3a55ffba2873adbc749a53301497b326589dbcf1"
      },
      "cell_type": "markdown",
      "source": "## Tokenization Methods"
    },
    {
      "metadata": {
        "_uuid": "430a5ad75ce75a1547db44e8ec76d869c8ff9538"
      },
      "cell_type": "markdown",
      "source": "**Approach 1: Keras Tokenizer **"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f3b901010fb6d95a83a679c120c78c926f9ed58d"
      },
      "cell_type": "code",
      "source": "X_train = train[\"question_text\"].values\ny_train = train[\"target\"].values\nX_test = test[\"question_text\"].values\n\nmax_features = 40000\nmaxlen = 70\nembed_size = 300\n\nthreshold = 0.33\n\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(all_text.values)) # it is helpful to use all data for competition purpose\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\nx_train = sequence.pad_sequences(X_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(X_test, maxlen=maxlen)\nword_index = tokenizer.word_index",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eac11aad6186c9c298ae53b7f796919529e37d77"
      },
      "cell_type": "code",
      "source": "terms = np.array(list(word_index.keys()))\nindices = np.array(list(word_index.values()))\ninverse_vocabulary = terms[np.argsort(indices)]\nlen(inverse_vocabulary)\nprint(inverse_vocabulary[X_train[1]])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b038306434ee6dbddfaae6744127312205267763"
      },
      "cell_type": "markdown",
      "source": "**Approach 2: Count vect removing stop words & punctuations and filtering words not consisting alphabets pattern**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6951937d56f89f688254b7061c6717d8556d9029"
      },
      "cell_type": "code",
      "source": "# initializing and fitting count vectorizer\n# max_features = 50000\n# embed_size = 300\n# cntVectBin = CountVectorizer(binary=False, stop_words=stop_words,\n#                              preprocessor=lambda x: \" \".join(text_to_word_sequence(x)),\n#                              token_pattern=\"[a-zA-Z]{2,}\",\n#                              max_features=max_features,\n#                              min_df=5, max_df=0.99, dtype=np.float32)\n# cntVectBin.fit(all_text)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4e5fed85ac1130fa4551e2f88ee94fd240159d99"
      },
      "cell_type": "code",
      "source": "# X_train = cntVectBin.transform(train[\"question_text\"])\n# X_test = cntVectBin.transform(test[\"question_text\"])\n\n# X_train = to_seq(X_train)\n# X_test = to_seq(X_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8b37a27787a50976639711ae0b89363f0d9434c6"
      },
      "cell_type": "code",
      "source": "# maxlen = 55\n# embed_size = 300\n# x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n# y_train = train[\"target\"].values\n# x_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n# word_index = cntVectBin.vocabulary_\n# max_features = len(word_index)\n# X_train[:10]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cea46f2a3d5f60d392eb6609f5f358c1d9507507"
      },
      "cell_type": "code",
      "source": "# terms = np.array(list(word_index.keys()))\n# indices = np.array(list(word_index.values()))\n# inverse_vocabulary = terms[np.argsort(indices)]\n# len(inverse_vocabulary)\n# print(inverse_vocabulary[X_train[1]])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4febe2c571653b6cc461df887f759e70544c123b"
      },
      "cell_type": "markdown",
      "source": "## **Handling missing vocab techniques**"
    },
    {
      "metadata": {
        "_uuid": "a06d63b5d19375c17606fec856633fb30772f6b9"
      },
      "cell_type": "markdown",
      "source": "**Approach 1: Making all missing zeros**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8acf49423213f8b582c22a70a2efaaeb13f54275"
      },
      "cell_type": "code",
      "source": "nb_words = min(max_features, len(word_index))\nembedding_matrix = np.zeros((nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embedding_index.get(word)\n    oov_words = []\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n    else:\n        oov_words.append(word)\nprint(len(word_index))\nprint(len(oov_words))\nprint(oov_words)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "84d43530ae30bc3226e8e26d5dec851b399e21d3"
      },
      "cell_type": "markdown",
      "source": "**Approach 2: Giving uniformly distributed random weights**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "458d5a63431c8fd3c786be29cfe40175ac378458"
      },
      "cell_type": "code",
      "source": "# all_embs = np.stack(embedding_index.values())\n# # emb_mean,emb_std = all_embs.mean(), all_embs.std()\n# emb_mean,emb_std = -0.005838498938828707, 0.4878219664096832 #result of above\n# embed_size = all_embs.shape[1]\n# nb_words = min(max_features, len(word_index))\n# embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n# oov_words = []\n# for word, i in word_index.items():\n#     if i >= max_features: continue\n#     embedding_vector = embedding_index.get(word)\n#     if embedding_vector is not None:\n#         embedding_matrix[i] = embedding_vector\n#     else:\n#         oov_words.append(word)\n# print(len(word_index))\n# print(len(oov_words))\n# print(oov_words)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "33bca339344442f7cc7f12178a3722d3f72ca7f8"
      },
      "cell_type": "code",
      "source": "threshold = 0.3\nclass F1Evaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=0)\n            y_pred = (y_pred > threshold).astype(int)\n            score = f1_score(self.y_val, y_pred)\n            print(\"\\n F1 Score - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n            \ndef as_keras_metric(method):\n    @functools.wraps(method)\n    def wrapper(self, args, **kwargs):\n        \"\"\" Wrapper for turning tensorflow metrics into keras metrics \"\"\"\n        value, update_op = method(self, args, **kwargs)\n        K.get_session().run(tf.local_variables_initializer())\n        with tf.control_dependencies([update_op]):\n            value = tf.identity(value)\n        return value\n    return wrapper\n\nauc_roc = as_keras_metric(tf.metrics.auc)\n# recall = as_keras_metric(tf.metrics.recall)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b8d17dc10c4628a16b61994d7cce343cb996a1cb"
      },
      "cell_type": "code",
      "source": "num_filters = 42\nfilter_sizes = [1,2,3,5]\n# cnn with above filters and max pooling\ndef get_model():    \n    inp = Input(shape=(maxlen, ))\n    x = Embedding(nb_words, embed_size,\n#                   embeddings_regularizer=regularizers.l1(0.0001),\n                  weights=[embedding_matrix],\n                  trainable=True)(inp)\n    x = Reshape((maxlen, embed_size, 1))(x)\n    \n    maxpools = []\n    for i in range(len(filter_sizes)):\n        conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], embed_size),\n                    kernel_initializer=he_normal(seed=333),\n#                     kernel_regularizer=regularizers.l2(0.1),\n                    activation='tanh')(x)\n#                                 kernel_initializer=lecun_normal(seed=42), activation='tanh')(x)\n\n        maxpools.append(MaxPool2D(pool_size=(maxlen - filter_sizes[i] + 1, 1))(conv))\n        \n    z = Concatenate(axis=1)(maxpools)   \n    z = Flatten()(z)\n    z = Dropout(0.3)(z)\n    \n    outp = Dense(1, activation=\"sigmoid\",kernel_regularizer=regularizers.l2(0.1))(z)\n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=[\"accuracy\"])\n\n    return model\n\n\n# nn with mean embeddings precalculated\ndef get_meanemb_model():\n    inp = Input(shape=(embed_size, ))\n    x = Reshape((1, embed_size, 1))(inp)\n    z = Flatten()(x)\n#     z = Dropout(0.1)(z)\n    hlayer1 = Dense(300, activation=\"relu\")(z)\n    hlayer2 = Dense(50, activation=\"relu\")(hlayer1)\n    outp = Dense(1, activation=\"sigmoid\")(hlayer2)\n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=[auc_roc])\n\n    return model\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ab8aedb1ef4c6503611e88c02b3e26ff6d5c1674"
      },
      "cell_type": "code",
      "source": "# well, don't know why class weight is hurting. May be due to batch gradient descent\ncls_wgt = class_weight.compute_class_weight('balanced',\n                                                 np.unique(y_train),\n                                                 y_train)\ncls_wgt = cls_wgt/cls_wgt[0]\nfraction = 0.25\nclass_weights = {\n    0: cls_wgt[0],\n    1: 0.35*cls_wgt[1]\n}\nclass_weights",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d8f78fe2b9bff9a73356560394706b5f98c07d06",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "\nX_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.9,\n                                              random_state=333)\nF1_Score = F1Evaluation(validation_data=(X_val, y_val), interval=1)\n# Set callback functions to early stop training and save the best model so far\ncallbacks = [F1_Score,\n             EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\nmodel = get_model()\nmodel.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c2ff6ed6e5f91c21a2ebad093f3d29d40f1a5618",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "batch_size = 256\nepochs = 10\nhist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs,initial_epoch=0,\n                 validation_data=(X_val, y_val), # shuffle=True,\n                 class_weight=class_weights,\n                 callbacks=callbacks, verbose=2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "37d6ff36c6f7b1aecb922ab820b8dcdd2edd8943",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "plot_history(hist, measure=\"acc\")\nmodel.load_weights(\"best_model.h5\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aef473c00b92f1ff09e55f0c68f4eeb0631206bc"
      },
      "cell_type": "code",
      "source": "val_pred = model.predict(X_val, batch_size=1024)\npred_test_y = model.predict(x_test, batch_size=1024)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a04911b68388a8ea8bafd199cdc8c9bddfff932f",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "print(val_pred.flatten())\nthresh = evaluatePredictions(y_val, val_pred.flatten(), silent=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9d0d214e56fd9488e5ba178424e7592b77e3d907"
      },
      "cell_type": "code",
      "source": "tr_pred = model.predict(X_tra, batch_size=1024)\nevaluatePredictions(y_tra, tr_pred.flatten(), silent=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "51ba32cbea04bdc70a6102448366e2c327149538"
      },
      "cell_type": "code",
      "source": "# serialize model to JSON\nmodel_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ff31d4e27dfd5944a3275b5b0bf3d52345c99def"
      },
      "cell_type": "code",
      "source": "pred_test_y = (pred_test_y > thresh).astype(int)\nprint(pred_test_y.flatten())\nsubmit_df = pd.DataFrame({\"qid\": test[\"qid\"], \"prediction\": pred_test_y.flatten()})\nprint(submit_df['prediction'].value_counts())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a5bb97abfc64ca2e7144f19dc3daf88cbab51cca"
      },
      "cell_type": "code",
      "source": "submit_df.to_csv(\"submission.csv\", index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "f46d409b327561920588a5890261eb99baca11fc"
      },
      "cell_type": "code",
      "source": "# # uberx not present in glove\n# ngrams = [\"uber\",\"ube\", 'ber', \"erx\", \"ub\", \"be\", \"er\", \"rx\"]\n# def handle_oov(ngrams):\n#     sumx = 0\n#     for token in ngrams:\n#         x1 = embedding_index.get(token)\n#         if x1 is not None:\n#             sumx += x1\n#     return sumx\n\n# x1 = handle_oov(ngrams)\n\n# x3 = embedding_index.get('random')   \n# x2 = embedding_index.get(\"uber\")\n# print(np.corrcoef(x1,x2)[0,1])\n# print(np.corrcoef(x3,x2)[0,1])\n# print(x2)\n# print(x1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e1388a34108558123d56164643c77ef408210918"
      },
      "cell_type": "code",
      "source": "# h1 = embedding_index.get(\"hello\")\n# h2 = w2v.get(\"hello\")\n# print(h1)\n# print(h2)\n# np.corrcoef(h1,h2)[0,1]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c989528c0cedf95943897a209a585007238bae5f",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "#Using stop words filter along with keras text to word sequence along with embeddings\nclass MeanEmbeddingVectorizer(object):\n    def __init__(self, embedding_index, stop_words=None, debug=False, useSpacy=False):\n        # todo documentation\n        self.embedding_index = embedding_index\n        if stop_words is None:\n            self.stop_words = set(stopwords.words('english'))\n        self.dim = 300 # we are using 300 dims embeddings\n        self.debug = debug\n        self.useSpacy = useSpacy\n        self.nlp = None\n        if self.useSpacy:\n            self.nlp = spacy.load('en_core_web_sm')\n    \n    def analyzer(self, X):\n        if self.useSpacy: #took long time\n            doc1 = nlp(X)\n            filtered_sentence = [token.lemma_ for token in doc1 if not token.is_stop and token.is_alpha and not token.dep_ == 'punct'and not token.lemma_ == '-PRON-']\n        else:\n            word_tokens = text_to_word_sequence(X) # tokenize given text into words\n            filtered_sentence = [w for w in word_tokens if not w in self.stop_words] # filter stop words\n        return set(filtered_sentence)\n\n        \n    def process(self, X):\n        \n        filtered_sentence = self.analyzer(X)\n        vectors = np.zeros(self.dim)\n        count = 0\n        # get vector from\n        for word in filtered_sentence:\n            w2v = self.embedding_index.get(word)\n            if w2v is not None:\n                vectors = vectors + w2v\n                count += 1\n            elif self.debug:\n                print(\"Word not found on embeddings: \", word)\n        if count > 0:\n            return vectors/count\n        else:\n            return vectors\n        \n    def fit(self,X,y):\n        return self\n    \n    def transform(self, X):\n        return np.array([self.process(words) for words in X ])         ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b646e359393c9e73d345acc354c49c21f6ef117a"
      },
      "cell_type": "code",
      "source": "# Embedding?",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1d6f8f5f86563b54e78b3b72f42532fdf7612bc0"
      },
      "cell_type": "code",
      "source": "# model.fit?",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}